---
title: 算法总结1—贝叶斯分类器
author: sword865
layout: post
date: 2009-09-05
url: /archives/21
posturl_add_url:
  - yes
categories:
  - 机器学习
tags:
  - 贝叶斯
---
&nbsp;这几天以很快的速度翻完了<集体智慧编程>,因为只是对里面的算法感兴趣,对那些web2.0的应用没什么感觉,所以很多地方都是一扫而过,现在按最后一章的顺序来对所有相关的算法作一个详细的复习&hellip;.
  
这个是第一篇&hellip;&hellip;贝叶斯分类器
  
**数学基础：****
  
**条件概率****
  
定义：设A, B是两个事件，且P(A)>0 称P(B∣A)=P(AB)/P(A)为在条件 A下发生的条件事件B发生的条件概率。
  
**乘法公式**
  
设P(A)>0，则有P(AB)=P(B∣A)P(A)
  
**全概率公式和贝叶斯公式**
  
定义 设S为试验E的样本空间，B1, B2, &hellip;Bn为E的一组事件，若BiBj=Ф, i&ne;j, i, j=1,
  
2, &hellip;,n; B1&cup;B2&cup;&hellip;&cup;Bn=S则称B1, B2, &hellip;, Bn为样本空间的一个划分。
  
**定理**
  
设试验E的样本空间为，A为E的事件，B1, B2, &hellip;,Bn为的一个划分，且P(Bi)>0
  
(i=1, 2, &hellip;n)，则P(A)=P(A∣B1)P(B1)+P(A∣B2)+ &hellip;+P(A∣Bn)P(Bn)称为全概率公式。
  
**定理**
  
设试验E的样本空间为S，A为E的事件，B1, B2, &hellip;,Bn为的一个划分，则P(Bi∣A)=P(A∣Bi)P(Bi)/&sum;P(B｜Aj)P(Aj)=P(B｜Ai)P(Ai)/P(B)称为贝叶斯公式。
  
说明：i，j均为下标，求和均是1到n 

**贝叶斯分类器原理：**
  
通过某些特征对不同的内容进行分类。
  
**特征的定义**
  
任何可以用来判断内容中具备或缺失的东西。如要对文档进行分类时，所谓的内容就是文档，特征就是文档中的单词(当然你也可以选择其他合理的东西)

当向贝叶斯分类器输入一个要进行分类的样本后，分类器会先对该样本进行分析，确定其特征，然后将根据这些特征时，计算样本属于各分类的概率。

朴素贝叶斯分类器的具体工作步骤：
  
1) 学习：向分类器输入一系列的训练数据，注意这些数据是包括其所属类别的，分类器将对训练数据进行分析，计算出
  
1.各个特征在各个分类中出现的概率(=某分类中具有该特征的数据数目 / 该分类数目)如先计算出各个单词在各种分类的文档出现的概率。
  
将该概率作为某分类下某特征出现的条件概率P(feature | category)
  
2.任选一个样本属于某分类的概率(=某分类文章数 / 文章总数)
  
记该概率为p(category)
  
在朴素的贝叶斯分类器中，我们假设将要组合的各个概率相互独立(当然，很多时候并非如此。我们有时会发现，当样本拥有某一特征时，则它就更可能拥有另一项特征。)
  
2)分类计算：在向分类器提供大量学习数据后，我们就可以用它对新的样本进行分类了。
  
首先对样本进行分析，找出其具有的各种特征，利用这些特征，我们来计算各个分类中出现该样本的概率p(sample
  
| category)。为了完成这一计算，我们只要简单将该分类下在该文档中出现过的特征出现的条件概率相乘即可。即&pi; (P(feature | category) 这里的feature是该样本拥有的所有特征。
  
但是，我们实际要计算的是P (category | sample),即给定样本属于某分类的条件概率。
  
这里，就用到了贝叶斯定理：P(A | B)=P(B | A)P(A) / P(B)
  
这里就是：P(category | sample)= P(sample |
  
category)P(category) / P(sample)
  
其中，P(sample | category)P(category)都已经在学习中计算得到，而P(sample)是样本出现 的概率，我们可以计算它，但是这是没有意义的，因为我们会计算出各个分类的条件概率，然后比较它们的大小确定样本所属的分类，而对各个条件概率而言p(sample)是完全一样的。所以，我们就省去了对它的计算。

这样，我们就可以确定一个样本的具体分类了。
  
当然，以上算法是有很多改进的，比如对各分类定义一个最小阀值，一旦没有达到任一分类的阀值，则归属为未知。
  
另外，针对朴素的贝叶斯分类器的缺点，还有很多其他的计算方法，如费舍尔方法。
  
**费舍尔方法**为文档中的每个特征都求得了分类的概率，然后又将这些概率组合起来，并判断其是否有可能构成一个随机集合。该方法还会返回每个分类的概率，这些概率彼此间可以进行比较。尽管这种方法更为复杂，但是因为它在为分类选择临界值（cutoff）时允许更大的灵活性，所以还是值得一学的。
  
不同于之前计算的P(feature | category)，费舍尔方法将计算P(category | feature)，即拥有某一特征的样本属于某分类的概率。首先用P(具有指定特征的属于某分类的样本数 |具有指定特征的样本数)作为这一概率，然后将特征出现在所有所有分类中的概率fresum=&sum;P(category | feature),最后用P(category | feature) / fresum作为新的P(category
  
| feature)。(这样修正后的值将比修正前更加有效)
  
然后，我们就可以通过将各个特征的概率组合起来得到待分类样本所属的类别了。我们可以通过简单的相乘完成这一目的，虽然这里也作了相互独立的假设，但也比之前号上很多了。费舍尔方法的计算方法是将所有概率相乘后取自然对数，在乘以-2.

**优缺点：**
  
朴素的贝叶斯分类器最大的优势是他在接受大量数据训练和查询时的高速度。尤其当训练量递增时更是如此(我们可以分多次的对其进行学习的训练，而一些其他的方法如决策树和支持向量机要一次传送整个训练数据集)
  
另一个优点是，其对分类器的学习情况有着比较简单的解释，我们可以简单的通过查询学习时计算的一些概率值来了解其分类原理。
  
朴素的贝叶斯分类最大的缺陷是它无法处理特征符合所产生的变化(即前面提到过的实际上难以满足的相互独立)

终于写完了&hellip;好累啊&hellip;还有8个算法&hellip;&hellip;


