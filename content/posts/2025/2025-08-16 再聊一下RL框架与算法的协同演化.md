+++
date = '2025-08-16T22:13:14+08:00'
title = '再聊一下RL框架与算法的协同演化'
author = "sword865"
type = "post"
tags = ["强化学习", "LLM"]
topics = ["分布式计算", "强化学习"]
+++

上一篇文章我们聊了一下Ray与LLM强化学习框架设计，探讨了其架构的演进，但是没有提到为什么框架会往这个方向逐渐演进而不是一开始就使用现在的设计。这里面自然有实践中不断优化的结果，但是也是和整个LLM RL需求的变化密切相关的。

因此，本文会主要讨论一下LLM强化学习中，算法与系统框架是如何相互影响、协同演化的。首先分析两个相对成熟的协同设计案例，然后讨论几个正在不够成熟、但是在笔者看来很有潜力优化方向。

# 算法与框架协同演化的典型案例

## 案例一：推理模型驱动的分离式架构设计
随着模型越来越强大，特别是像DeepSeek-R1这类“慢思考”模型，它们在推理时生成的文本长度往往很长，且每个样本的长度差异巨大。在传统的同步、在线（On-Policy）RL设置中（例如经典的PPO），这会造成严重的吞吐量瓶颈：所有并行的环境必须等待最长的那个任务完成，才能进入下一次迭代。这导致了大量的计算资源闲置，极大地拖慢了训练速度。

为了解决这个问题，研究者们开始拥抱异步RL框架。这类框架的核心思想是将经验生成（Experience Generation）与策略优化（Policy Optimization）解耦。例如，像AsyncFlow和AReaL这样的系统，将RL训练重组为生产者-消费者流水线：

生产者（Rollout Workers）: 大量的worker持续不断地生成经验数据，无需彼此等待。
消费者（Trainer Workers）: 训练器异步地从一个共享的缓冲区中获取数据进行模型更新。
这种“流式RL”（Streaming RL）的方法，避免了慢速任务阻塞整个流程，保证了所有计算设备都能保持繁忙，从而显著提升了训练吞吐量。这正是算法（转向异步、离线（Off-Policy）的变种）与系统（采用生产者-消费者模式）协同设计的一个绝佳范例。

## 案例二：MoE架构与训练推理引擎的精度对齐挑战

另一个挑战出现在系统层面，尤其是在使用超大模型或混合专家（MoE）模型时：训练引擎和推理引擎之间的不匹配。

在RL微调中，为了效率，模型的推理（生成）过程可能会在专门的推理引擎上执行（例如使用vLLM），而梯度计算则在训练后端（例如DeepSpeed）上进行。然而，两者在数值精度（如FP16 vs INT8）、算子融合、批处理调度或MoE门控行为上的差异，可能导致rollout数据和训练计算之间的分布漂移。

举个例子，为了加速生成，我们可能会使用8位量化的推理，这意味着采样是在一个与高精度训练策略略有不同的策略下进行的。这种“rollout-训练精度差距”会降低训练的稳定性，实际上使过程更偏向于离线（Off-Policy）。

为了应对这个问题，研究者们再次采用了算法和系统协同设计。

系统层面：工程上的努力致力于对齐训练和推理的底层实现，例如统一Kernel实现、确保MoE路由行为一致等。
算法层面：FlashRL等工作提出了一种名为**截断重要性采样（Truncated Importance Sampling, TIS）**的技术。它通过对更新进行重新加权，来修正量化推理（用于采样）和全精度模型（用于优化）之间的策略差异。通过TIS，即使是高度量化的rollout数据（如INT8/FP8）也能被用于训练，而不会损害最终效果。
通过结合算法调整（如解耦的PPO、重要性采样校正）和底层优化（量化感知核函数），研究者们成功地弥合了训练和推理之间的鸿沟。

# （可能的）发展趋势：协同设计的新兴挑战与机遇

## 方向一：Agent RL: 环境管理与过程奖励
随着LLM越来越多地被用作自主智能体（Agent）——能够与工具、API或环境进行多轮交互——新的RL挑战也随之而来。

目前按个人的理解，常见的Agent RL主要可以分为两种类型：

1. Sandbox + Browser
2. MCP + Tooluse

长期信用分配（Long-Horizon Credit Assignment）：当一个智能体为了完成一个复杂任务（例如编写一个软件）而执行数千步操作时，等到最后才给予一个成败的奖励信号是极其低效的。因此，**过程奖励（Process-level Rewards）或智能体引导（Agent Guidance）**变得至关重要。例如，Agent-RLVR框架在RL过程中加入了高层提示、动态错误反馈等“教学式”奖励，显著提升了智能体在复杂编码任务上的成功率。

状态管理与可观测性（State Management & Observability）：LLM智能体的“状态”可能包含整个对话历史、工具输出以及其内部的中间思考。传统的RL框架难以处理如此复杂的状态。新的研究方向包括情景记忆模块和基于摘要的状态压缩。AGILE等框架发现，为智能体配备明确的记忆和反思模块，对于提升其性能至关重要。


语言媒介的环境（Language-Mediated Environments）：智能体的环境不再是简单的模拟器，而是一个需要通过语言进行交互的动态系统，例如一个Web浏览器、一个代码沙箱，甚至是其他AI智能体。微软的ARTIST框架就将工具使用（如调用计算器或搜索引擎）整合到了RL循环中，智能体必须学习何时以及如何调用工具来解决问题。

为了支持这些需求，RL的基础设施也在演进。未来的框架需要更加模块化，能够轻松地接入各种自定义的环境模拟器或工具API。像模型上下文协议（Model-Context Protocol, MCP）这样的标准化接口，也为智能体学习有效的工具调用行为提供了基础。


## 方向二：生成式奖励模型与进一步的分离式架构
在对齐研究中，一个令人兴奋的进展是生成式奖励模型（Generative Reward Models）的出现。传统的奖励模型通常对一个输出给出一个标量分数，而生成式奖励模型（通常也是一个LLM）则可以生成长篇的评估或推理轨迹来给出判断，也就是所谓的LLM-as-a-judge或RLAIF（RL from AI Feedback）。

这种范式有几个好处：

更丰富的监督信号：奖励模型生成的“评语”可以精确地指出答案好在哪里或坏在哪里，比单一的数字分数信息量大得多。
更好的泛化性：通过在海量数据上预训练，生成式奖励模型拥有强大的通用知识，在面对分布外的提示时表现更鲁棒。
促进新的训练架构：我们可以设计出策略模型和奖励模型进行“对话”的训练流程。例如，策略模型提出一个解决方案，奖励模型（作为批评家）给出一段文字评论，然后这段评论被用来进一步优化策略模型。
当然，这也带来了新的挑战，比如策略模型可能会学会“钻空子”，去优化评语的措辞而不是真正的质量（即奖励攻击）。为了缓解这个问题，研究者们正在探索CoT增强的奖励建模等技术。

总的来说，生成式奖励模型代表了RLHF与LLM内部推理能力的融合，它推动了RL训练流水线向着支持多个大型模型（策略模型、奖励模型等）并行交互的方向发展。

其他还有一些比如多模态的强化学习，不是很熟，就不写了。

## 方向三：更进一步的样本效率提升
不论是前面提到的Agent RL还是生成式奖励模型，都对样本的rollout效率提出了巨大的挑战，因此整个算法大概也会比现在的offline更加offline。那么如何在offline的同时保证训练的有效性就是一个非常重要的问题。


# 总结：算法与系统的协同演化
上面的例子清晰地展示了LLM强化学习中，新算法思想和新系统设计是如何齐头并进的。我们正在见证一场算法和框架的真正协同演化。

事实上，LLM强化学习框架一直与传统的强化学习框架有较大的区别：

算法创新推动系统变革：异步离线策略、解耦的PPO、截断重要性采样等算法，都是为了解决大规模分布式系统中的瓶颈而提出的。
系统进步赋能新算法：流式数据流水线、统一的工具接口、多模型训练工作流等系统设计，则为更复杂的算法（如智能体学习、生成式奖励）的实现提供了可能。
这个良性循环正在不断加速。在不久的将来，我们可以期待看到更加分布式、工具感知、模型无关的RL框架，让研究者可以轻松地替换更大的模型和更丰富的环境。同样，新的算法在设计之初就会考虑到系统的约束，例如如何在大规模并行下保持稳定。

最终的目标是创造出既强大又高效的新一代推理AI智能体。每一次算法上的进步都将推动框架的迭代，而每一次框架能力的飞跃又将催生新的算法——这场协同演化，正在将我们带向更通用、更强大的AI未来。