---
title: Google在KDD2013上关于CTR的一篇论文
author: sword865
type: post
date: 2013-08-03
url: /archives/6
posturl_add_url:
  - yes
categories:
  - 机器学习
tags:
  - CTR
  - logistic
---
最近在做CTR，刚好Google在KDD发了一篇文章，讲了他们的一些尝试，总结一下：

先是一些公式的符号说明：

[<img style="background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;" title="image" src="http://images.cnitblog.com/blog/52809/201308/04011358-949b9f116ca94563ab5a0efa047697db.png" alt="image" width="447" height="98" border="0" />][1]

一、优化算法

CTR中经常用Logistic regression进行训练，一个常用的Loss Function为

[<img style="background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;" title="image" src="http://images.cnitblog.com/blog/52809/201308/04011358-03407ff016d04735b28103abc573d428.png" alt="image" width="329" height="34" border="0" />][2]

Online gradient descent(OGD)是一个常用的优化方法，但是在加上L1正则化后，这种方法不能产生有效的稀疏模型。相比之下 Regularized Dual Averaging (RDA)拥有更好的稀疏性，但是精度不如OGD好。

FTRL-Proximal 方法可以同时得到稀疏性与精确性，不同于OGD的迭代步骤：

[<img style="background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;" title="image" src="http://images.cnitblog.com/blog/52809/201308/04011359-6f55d3e1f55f4389979bec14102120b5.png" alt="image" width="187" height="39" border="0" />][3]

其中$\eta_t$是一个非增的学习率

FTRL-Proximal通过下式迭代：

[<img style="background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;" title="image" src="http://images.cnitblog.com/blog/52809/201308/04011359-37b635900b57402a9efc1d0fbf12e7d2.png" alt="image" width="482" height="60" border="0" />][4]

&nbsp;

参数$\sigma$是学习率，一般我们有 ${\sum_{s=1}^t\sigma_s=\frac{1}{\eta_t}}$。

更新公式：

[<img style="background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border-width: 0px;" title="image" src="http://images.cnitblog.com/blog/52809/201308/04011359-ade11cdad0064b918877e4fbdd0babb1.png" alt="image" width="377" height="74" border="0" />][5]

算法如下：

[<img style="background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;" title="image" src="http://images.cnitblog.com/blog/52809/201308/04011400-cbd594724918403f8c25f0a88ee489ae.png" alt="image" width="428" height="368" border="0" />][6]

这里多个一个 $\lambda_2$ 是一个L2正则化参数。

二、学习率

$$\displaystyle \eta_t=\frac{1}{\sqrt{t}}$$

由于在求解时，这样，对每一个坐标我们都使用了同样的参数，这样一些没有使用的坐标的参数也会下降，显然这不太合理。

一个近似最优的选择是：

[<img style="background-image: none; padding-top: 0px; padding-left: 0px; margin: 0px; display: inline; padding-right: 0px; border: 0px;" title="image" src="http://images.cnitblog.com/blog/52809/201308/04011400-76896e8c7af94f00a204a895b9e243d8.png" alt="image" width="244" height="78" border="0" />][7]g是梯度向量

&nbsp;

三、存储空间

1.特征选择

在CTR中，跟多特征仅仅出现了一次(In fact, in some of our models, half the unique features occur only once in the entire training set of billions of examples)，这样特征几乎没有什么用，但是存储起来非常浪费空间。L1正则化虽然解决了一些问题，但是这样降低了一些精度，因此另一个选择是

probabilistic feature inclusion，这种方法中，一个特征第一次出现时，会以一定的概率被保存使用。关于这个概率Google尝试了两种方法：

Poisson Inclusion：以概率p增加特征，这样一般特征被加入就需要出现1/p次

Bloom Filter Inclusion：用一系列的Bloom flters来检测特征的前n次出现，一旦检测到出现了n次（因为BF有冲突，所以实际可能少于n），就加入模型并用在后面的训练中。

2.系数编码

因为大部分系数都在-2和2之间，因此使用了定点的q2.13编码，同时也保证了小数的一些精度。编码包括一位的符号，2位的整数和13位的小数。

因此误差可能在OGD算法中发散，因此使用了一个简单的随机取整策略：

[<img style="background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;" title="image" src="http://images.cnitblog.com/blog/52809/201308/04011400-eea1a760e2124dcca8efa0e15f16c808.png" alt="image" width="285" height="46" border="0" />][8]R是一个0到1的随机整数。

3.多个相似模型的训练

在测试一些超参数的影响时，同时训练多个模型非常有用。观察发现，有些数据可以被多个模型共用，但是另外一些（如系数）不能，如果把模型的系数存在一个HASH表里，就可以让多个变体同时使用这些参数，比如学习率。

4.单值结构

有时我们想训练一些模型，他们之间只是删除或增加了一些特征。单值特征为每一个特征存了一个权重，权重被所有有该特征的模型共享，学习方法如下：

在OGD更新中，每个模型用他自己的的那部分特征计算一个Loss, 然后对每一个特征，每一个模型计算一个新的系数，最后把所有值平均后存为单值。该单值下一步被所有模型使用。

5.计数与梯度

假设所有事件包括统一特征的概率相同（一个粗糙但是有效的估计），其中出现了P次，没有出现N次，那么出现的概率就是p=P/(N+P),那么在logistic regression中，正事件的导数是p-1,负事件p，梯度的平方和就是：

[<img style="background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;" title="image" src="http://images.cnitblog.com/blog/52809/201308/04011401-7fc7cc332f634239bff48a489dd1c997.png" alt="image" width="389" height="145" border="0" />][9]

&nbsp;

6.采样训练数据：

CTR中的负样本远高与正样本，因此采样的数据包括满足所有的正样本和部分负样本：

[<img style="background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;" title="image" src="http://images.cnitblog.com/blog/52809/201308/04011401-10e2c56d7cf94e08ab2d89e884481391.png" alt="image" width="421" height="64" border="0" />][10]

在训练中给正样本1的权重，负样本1/r的权重以避免模型结果出错。权重乘如Loss Function对应项中。

&nbsp;

[<img style="background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;" title="image" src="http://images.cnitblog.com/blog/52809/201308/04011401-cf3e0ae967e148a7ae5365ccc0f7fc51.png" alt="image" width="349" height="64" border="0" />][11]

[<img style="background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;" title="image" src="http://images.cnitblog.com/blog/52809/201308/04011402-a833a1c25f19478f92c19b6b06db8ed0.png" alt="image" width="467" height="52" border="0" />][12]

四、模型评价1

1.进度验证(Progressive Validation)

因为计算梯度的同时需要计算预测值，因此可以收集这些值。在线的loss反映了算法的表现&#8212;他度量了训练一个数据前得到的预测结果。这样也使得所有数据被同时作用训练集和测试集使用。

2.可视化加强理解

[<img style="background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;" title="image" src="http://images.cnitblog.com/blog/52809/201308/04011404-eaf5de3e66314e82aa675ba92be4fcd8.png" alt="image" width="562" height="267" border="0" />][13]

&nbsp;

上图对query进行了切片后，将两个模型与一个控制模型模型进行了比较。度量用颜色表示，每行是一个模型，每列是一个切片。

五、置信估计

[<img style="background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;" title="image" src="http://images.cnitblog.com/blog/52809/201308/04011405-3f7f56dae9a34817a96f412ad13a8dcd.png" alt="image" width="434" height="119" border="0" />][14]

六、预测矫正

矫正的数据[<img style="background-image: none; padding-top: 0px; padding-left: 0px; display: inline; padding-right: 0px; border: 0px;" title="image" src="http://images.cnitblog.com/blog/52809/201308/04011405-93a76b43c030441c9295ded6239bbb1c.png" alt="image" width="71" height="34" border="0" />][15]p是模型预测的CTR，d是一些训练数据。

一个常用矫正：[<img style="background-image: none; padding-top: 0px; padding-left: 0px; margin: 0px; display: inline; padding-right: 0px; border: 0px;" title="image" src="http://images.cnitblog.com/blog/52809/201308/04011406-e83e0f3784944b10812b43ca5c74a2c4.png" alt="image" width="112" height="43" border="0" />][16]

两个参数可以用Poisson regression在数据上训练。

<p style="margin:0;padding:0;height:1px;overflow:hidden;">
  <a href="http://www.wumii.com/widget/relatedItems" style="border:0;"><img src="http://static.wumii.cn/images/pixel.png" alt="无觅相关文章插件，快速提升流量" style="border:0;padding:0;margin:0;" /></a>
</p>

 [1]: http://images.cnitblog.com/blog/52809/201308/04011358-7d9b9023e473455e9c21b18486a58fa8.png
 [2]: http://images.cnitblog.com/blog/52809/201308/04011358-9e64ee86e7f14dbd8a3da9f8a2bb7c13.png
 [3]: http://images.cnitblog.com/blog/52809/201308/04011358-2e856d4483c64ef0a668ee8c8df2a548.png
 [4]: http://images.cnitblog.com/blog/52809/201308/04011359-900ea01f64a547bd99cefe8e375a3a9d.png
 [5]: http://images.cnitblog.com/blog/52809/201308/04011359-5cd9cb60ba40409ca3ef8988e2e11f47.png
 [6]: http://images.cnitblog.com/blog/52809/201308/04011359-8100f10672694bffa4e1a46fc3b01719.png
 [7]: http://images.cnitblog.com/blog/52809/201308/04011400-7ee5511abe4e40a38bf4950df39c1335.png
 [8]: http://images.cnitblog.com/blog/52809/201308/04011400-578afca151244861974554d833d64ade.png
 [9]: http://images.cnitblog.com/blog/52809/201308/04011401-abaa4c9233c443179f17fc62822d599f.png
 [10]: http://images.cnitblog.com/blog/52809/201308/04011401-46a2083bf5904c3c986dcacfc63bb0e2.png
 [11]: http://images.cnitblog.com/blog/52809/201308/04011401-301af9b06f524c5d947e9739b2e16cf8.png
 [12]: http://images.cnitblog.com/blog/52809/201308/04011402-eeef6de793ad4baaae18d7251f6d482d.png
 [13]: http://images.cnitblog.com/blog/52809/201308/04011403-ea4fafaae6b5448ca3826eb98a107bf1.png
 [14]: http://images.cnitblog.com/blog/52809/201308/04011405-1ba03bff12c740beb22d7c59e6e6d899.png
 [15]: http://images.cnitblog.com/blog/52809/201308/04011405-372b84501cc84940a965b739cf97a9d9.png
 [16]: http://images.cnitblog.com/blog/52809/201308/04011406-75988e6dea0d4d83b698e96019c9753b.png

 

 